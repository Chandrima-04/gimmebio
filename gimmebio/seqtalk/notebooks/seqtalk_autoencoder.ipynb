{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from argparse import Namespace\n",
    "\n",
    "from gimmebio.seqtalk.autoencode.architecture import build_explicit_encoder\n",
    "from gimmebio.seqtalk.autoencode.training import build_feed_dict\n",
    "from gimmebio.seqtalk.seq_data import FastqSeqData\n",
    "\n",
    "EXAMPLE_FASTQ = '/home/dcdanko/Dropbox/10M.data2_accctcct.fq'\n",
    "data_source = FastqSeqData(EXAMPLE_FASTQ, seq_len=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AutoEncoder(torch.nn.Module):\n",
    "    \n",
    "    def __init__(self, input_length, alphabet_size, dense_dims):\n",
    "        super(AutoEncoder, self).__init__()\n",
    "        self.encode1 = torch.nn.Linear(input_length * alphabet_size, dense_dims)\n",
    "        self.encode2 = torch.nn.Linear(dense_dims, input_length)\n",
    "        self.decode1 = torch.nn.Linear(input_length, dense_dims)\n",
    "        self.decode2 = torch.nn.Linear(dense_dims, input_length * alphabet_size)\n",
    "        \n",
    "    def encode(self, x):\n",
    "        encoded = self.encode1(x).clamp(min=0)\n",
    "        encoded = self.encode2(encoded).clamp(min=0)\n",
    "        return encoded\n",
    "\n",
    "    def decode(self, x):\n",
    "        decoded = self.decode1(x).clamp(min=0)\n",
    "        decoded = self.decode2(decoded).clamp(min=0)\n",
    "        decoded = decoded / torch.max(decoded)\n",
    "        return decoded\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.decode(self.encode(x))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0 loss: tensor(12.2511)\n",
      "epoch: 1 loss: tensor(11.5762)\n",
      "epoch: 3 loss: tensor(11.1388)\n",
      "epoch: 4 loss: tensor(10.7065)\n",
      "epoch: 5 loss: tensor(10.2236)\n",
      "epoch: 6 loss: tensor(8.7900)\n",
      "epoch: 7 loss: tensor(8.3072)\n",
      "epoch: 8 loss: tensor(8.1297)\n",
      "epoch: 9 loss: tensor(7.9463)\n",
      "epoch: 10 loss: tensor(7.7680)\n",
      "epoch: 11 loss: tensor(7.6292)\n",
      "epoch: 12 loss: tensor(7.5076)\n",
      "epoch: 13 loss: tensor(7.4406)\n",
      "epoch: 14 loss: tensor(7.2356)\n",
      "epoch: 15 loss: tensor(7.0785)\n",
      "epoch: 16 loss: tensor(6.9889)\n",
      "epoch: 17 loss: tensor(6.8844)\n",
      "epoch: 18 loss: tensor(6.7975)\n",
      "epoch: 19 loss: tensor(6.7672)\n",
      "epoch: 20 loss: tensor(6.7397)\n",
      "epoch: 21 loss: tensor(6.7220)\n",
      "epoch: 22 loss: tensor(6.6884)\n",
      "epoch: 23 loss: tensor(6.6783)\n",
      "epoch: 24 loss: tensor(6.6618)\n",
      "epoch: 25 loss: tensor(6.6349)\n",
      "epoch: 26 loss: tensor(6.6260)\n",
      "epoch: 27 loss: tensor(6.6054)\n",
      "epoch: 28 loss: tensor(6.5892)\n",
      "epoch: 29 loss: tensor(6.5800)\n",
      "epoch: 30 loss: tensor(6.5622)\n",
      "epoch: 31 loss: tensor(6.5435)\n",
      "epoch: 32 loss: tensor(6.5461)\n",
      "epoch: 33 loss: tensor(6.5239)\n",
      "epoch: 34 loss: tensor(6.5164)\n",
      "epoch: 35 loss: tensor(6.5088)\n",
      "epoch: 36 loss: tensor(6.4938)\n",
      "epoch: 37 loss: tensor(6.4648)\n",
      "epoch: 38 loss: tensor(6.4211)\n",
      "epoch: 39 loss: tensor(6.4068)\n",
      "epoch: 40 loss: tensor(6.4093)\n",
      "epoch: 41 loss: tensor(6.3883)\n",
      "epoch: 42 loss: tensor(6.3853)\n",
      "epoch: 43 loss: tensor(6.3823)\n",
      "epoch: 44 loss: tensor(6.3734)\n",
      "epoch: 45 loss: tensor(6.3640)\n",
      "epoch: 46 loss: tensor(6.3575)\n",
      "epoch: 47 loss: tensor(6.3554)\n",
      "epoch: 48 loss: tensor(6.3602)\n",
      "epoch: 49 loss: tensor(6.3530)\n",
      "epoch: 50 loss: tensor(6.3494)\n",
      "epoch: 51 loss: tensor(6.3420)\n",
      "epoch: 52 loss: tensor(6.3384)\n",
      "epoch: 53 loss: tensor(6.3294)\n",
      "epoch: 54 loss: tensor(6.2944)\n",
      "epoch: 55 loss: tensor(6.2481)\n",
      "epoch: 56 loss: tensor(6.2300)\n",
      "epoch: 57 loss: tensor(6.2137)\n",
      "epoch: 58 loss: tensor(6.2054)\n",
      "epoch: 59 loss: tensor(6.1790)\n",
      "epoch: 60 loss: tensor(6.1365)\n",
      "epoch: 61 loss: tensor(6.1071)\n",
      "epoch: 62 loss: tensor(6.0890)\n",
      "epoch: 63 loss: tensor(6.0879)\n",
      "epoch: 64 loss: tensor(6.0793)\n",
      "epoch: 65 loss: tensor(6.0801)\n",
      "epoch: 66 loss: tensor(6.0829)\n",
      "epoch: 67 loss: tensor(6.0723)\n",
      "epoch: 68 loss: tensor(6.0657)\n",
      "epoch: 69 loss: tensor(6.0658)\n",
      "epoch: 70 loss: tensor(6.0600)\n",
      "epoch: 71 loss: tensor(6.0663)\n",
      "epoch: 72 loss: tensor(6.0531)\n",
      "epoch: 73 loss: tensor(6.0485)\n",
      "epoch: 74 loss: tensor(6.0435)\n",
      "epoch: 75 loss: tensor(6.0429)\n",
      "epoch: 76 loss: tensor(6.0267)\n",
      "epoch: 77 loss: tensor(6.0275)\n",
      "epoch: 78 loss: tensor(6.0279)\n",
      "epoch: 79 loss: tensor(6.0199)\n",
      "epoch: 80 loss: tensor(6.0213)\n",
      "epoch: 81 loss: tensor(6.0249)\n",
      "epoch: 82 loss: tensor(6.0207)\n",
      "epoch: 83 loss: tensor(6.0144)\n",
      "epoch: 84 loss: tensor(6.0194)\n",
      "epoch: 85 loss: tensor(6.0105)\n",
      "epoch: 86 loss: tensor(6.0051)\n",
      "epoch: 87 loss: tensor(6.0055)\n",
      "epoch: 88 loss: tensor(6.0073)\n",
      "epoch: 89 loss: tensor(6.0139)\n",
      "epoch: 90 loss: tensor(6.0056)\n",
      "epoch: 91 loss: tensor(5.9988)\n",
      "epoch: 92 loss: tensor(6.0035)\n",
      "epoch: 93 loss: tensor(6.0005)\n",
      "epoch: 94 loss: tensor(6.0038)\n",
      "epoch: 95 loss: tensor(5.9957)\n",
      "epoch: 96 loss: tensor(5.9954)\n",
      "epoch: 97 loss: tensor(5.9942)\n",
      "epoch: 98 loss: tensor(5.9981)\n",
      "epoch: 99 loss: tensor(5.9924)\n",
      "epoch: 100 loss: tensor(5.9915)\n",
      "epoch: 101 loss: tensor(5.9913)\n",
      "epoch: 102 loss: tensor(5.9853)\n",
      "epoch: 103 loss: tensor(5.9880)\n",
      "epoch: 104 loss: tensor(5.9865)\n",
      "epoch: 105 loss: tensor(5.9885)\n",
      "epoch: 106 loss: tensor(5.9905)\n",
      "epoch: 107 loss: tensor(5.9901)\n",
      "epoch: 108 loss: tensor(5.9898)\n",
      "epoch: 109 loss: tensor(5.9884)\n",
      "epoch: 110 loss: tensor(5.9821)\n",
      "epoch: 111 loss: tensor(5.9827)\n",
      "epoch: 112 loss: tensor(5.9895)\n",
      "epoch: 113 loss: tensor(5.9813)\n",
      "epoch: 114 loss: tensor(5.9763)\n",
      "epoch: 115 loss: tensor(5.9792)\n",
      "epoch: 116 loss: tensor(5.9828)\n",
      "epoch: 117 loss: tensor(5.9812)\n",
      "epoch: 118 loss: tensor(5.9774)\n",
      "epoch: 119 loss: tensor(5.9801)\n",
      "epoch: 120 loss: tensor(5.9786)\n",
      "epoch: 121 loss: tensor(5.9754)\n",
      "epoch: 122 loss: tensor(5.9729)\n",
      "epoch: 123 loss: tensor(5.9670)\n",
      "epoch: 124 loss: tensor(5.9357)\n",
      "epoch: 125 loss: tensor(5.9056)\n",
      "epoch: 126 loss: tensor(5.8977)\n",
      "epoch: 127 loss: tensor(5.8910)\n",
      "epoch: 128 loss: tensor(5.8924)\n",
      "epoch: 129 loss: tensor(5.8835)\n",
      "epoch: 130 loss: tensor(5.8825)\n",
      "epoch: 131 loss: tensor(5.8775)\n",
      "epoch: 132 loss: tensor(5.8758)\n",
      "epoch: 133 loss: tensor(5.8747)\n",
      "epoch: 134 loss: tensor(5.8727)\n",
      "epoch: 135 loss: tensor(5.8699)\n",
      "epoch: 136 loss: tensor(5.8696)\n",
      "epoch: 137 loss: tensor(5.8688)\n",
      "epoch: 138 loss: tensor(5.8665)\n",
      "epoch: 139 loss: tensor(5.8656)\n",
      "epoch: 140 loss: tensor(5.8602)\n",
      "epoch: 141 loss: tensor(5.8581)\n",
      "epoch: 142 loss: tensor(5.8500)\n",
      "epoch: 143 loss: tensor(5.8536)\n",
      "epoch: 144 loss: tensor(5.8525)\n",
      "epoch: 145 loss: tensor(5.8438)\n",
      "epoch: 146 loss: tensor(5.8542)\n",
      "epoch: 147 loss: tensor(5.8398)\n",
      "epoch: 148 loss: tensor(5.8489)\n",
      "epoch: 149 loss: tensor(5.8422)\n",
      "epoch: 150 loss: tensor(5.8445)\n",
      "epoch: 151 loss: tensor(5.8401)\n",
      "epoch: 152 loss: tensor(5.8342)\n",
      "epoch: 153 loss: tensor(5.8318)\n",
      "epoch: 154 loss: tensor(5.8347)\n",
      "epoch: 155 loss: tensor(5.8234)\n",
      "epoch: 156 loss: tensor(5.8260)\n",
      "epoch: 157 loss: tensor(5.8222)\n",
      "epoch: 158 loss: tensor(5.8191)\n",
      "epoch: 159 loss: tensor(5.8236)\n",
      "epoch: 160 loss: tensor(5.8203)\n",
      "epoch: 161 loss: tensor(5.8204)\n",
      "epoch: 162 loss: tensor(5.8227)\n",
      "epoch: 163 loss: tensor(5.8274)\n",
      "epoch: 164 loss: tensor(5.8203)\n",
      "epoch: 165 loss: tensor(5.8114)\n",
      "epoch: 166 loss: tensor(5.8151)\n",
      "epoch: 167 loss: tensor(5.8186)\n",
      "epoch: 168 loss: tensor(5.8146)\n",
      "epoch: 169 loss: tensor(5.8147)\n",
      "epoch: 170 loss: tensor(5.8150)\n",
      "epoch: 171 loss: tensor(5.8094)\n",
      "epoch: 172 loss: tensor(5.8116)\n",
      "epoch: 173 loss: tensor(5.8102)\n",
      "epoch: 174 loss: tensor(5.8082)\n",
      "epoch: 175 loss: tensor(5.8046)\n",
      "epoch: 176 loss: tensor(5.8083)\n",
      "epoch: 177 loss: tensor(5.8090)\n",
      "epoch: 178 loss: tensor(5.8024)\n",
      "epoch: 179 loss: tensor(5.8065)\n",
      "epoch: 180 loss: tensor(5.8048)\n",
      "epoch: 181 loss: tensor(5.8058)\n",
      "epoch: 182 loss: tensor(5.7964)\n",
      "epoch: 183 loss: tensor(5.8020)\n",
      "epoch: 184 loss: tensor(5.8080)\n",
      "epoch: 185 loss: tensor(5.8048)\n",
      "epoch: 186 loss: tensor(5.8008)\n",
      "epoch: 187 loss: tensor(5.7970)\n",
      "epoch: 188 loss: tensor(5.8000)\n",
      "epoch: 189 loss: tensor(5.7956)\n",
      "epoch: 190 loss: tensor(5.7983)\n",
      "epoch: 191 loss: tensor(5.7948)\n",
      "epoch: 192 loss: tensor(5.8011)\n",
      "epoch: 193 loss: tensor(5.7960)\n",
      "epoch: 194 loss: tensor(5.7913)\n",
      "epoch: 195 loss: tensor(5.7996)\n",
      "epoch: 196 loss: tensor(5.7971)\n",
      "epoch: 197 loss: tensor(5.7928)\n",
      "epoch: 198 loss: tensor(5.7924)\n",
      "epoch: 199 loss: tensor(5.7989)\n",
      "epoch: 200 loss: tensor(5.7945)\n",
      "epoch: 201 loss: tensor(5.7873)\n",
      "epoch: 202 loss: tensor(5.7944)\n",
      "epoch: 203 loss: tensor(5.7923)\n",
      "epoch: 204 loss: tensor(5.7917)\n",
      "epoch: 205 loss: tensor(5.7911)\n",
      "epoch: 206 loss: tensor(5.7849)\n",
      "epoch: 207 loss: tensor(5.7913)\n",
      "epoch: 208 loss: tensor(5.7910)\n",
      "epoch: 209 loss: tensor(5.7938)\n",
      "epoch: 210 loss: tensor(5.7921)\n",
      "epoch: 211 loss: tensor(5.7872)\n",
      "epoch: 212 loss: tensor(5.7873)\n",
      "epoch: 213 loss: tensor(5.7824)\n",
      "epoch: 214 loss: tensor(5.7876)\n",
      "epoch: 215 loss: tensor(5.7889)\n",
      "epoch: 216 loss: tensor(5.7869)\n",
      "epoch: 217 loss: tensor(5.7860)\n",
      "epoch: 218 loss: tensor(5.7862)\n",
      "epoch: 219 loss: tensor(5.7854)\n",
      "epoch: 220 loss: tensor(5.7842)\n",
      "epoch: 221 loss: tensor(5.7859)\n",
      "epoch: 222 loss: tensor(5.7860)\n",
      "epoch: 223 loss: tensor(5.7831)\n",
      "epoch: 224 loss: tensor(5.7865)\n",
      "epoch: 225 loss: tensor(5.7815)\n",
      "epoch: 226 loss: tensor(5.7821)\n",
      "epoch: 227 loss: tensor(5.7832)\n",
      "epoch: 228 loss: tensor(5.7773)\n",
      "epoch: 229 loss: tensor(5.7797)\n",
      "epoch: 230 loss: tensor(5.7812)\n",
      "epoch: 231 loss: tensor(5.7802)\n",
      "epoch: 232 loss: tensor(5.7796)\n",
      "epoch: 233 loss: tensor(5.7734)\n",
      "epoch: 234 loss: tensor(5.7776)\n",
      "epoch: 235 loss: tensor(5.7768)\n",
      "epoch: 236 loss: tensor(5.7780)\n",
      "epoch: 237 loss: tensor(5.7703)\n",
      "epoch: 238 loss: tensor(5.7746)\n",
      "epoch: 239 loss: tensor(5.7751)\n",
      "epoch: 240 loss: tensor(5.7782)\n",
      "epoch: 241 loss: tensor(5.7788)\n",
      "epoch: 242 loss: tensor(5.7809)\n",
      "epoch: 243 loss: tensor(5.7726)\n",
      "epoch: 244 loss: tensor(5.7674)\n",
      "epoch: 245 loss: tensor(5.7732)\n",
      "epoch: 246 loss: tensor(5.7775)\n",
      "epoch: 247 loss: tensor(5.7764)\n",
      "epoch: 248 loss: tensor(5.7747)\n",
      "epoch: 249 loss: tensor(5.7726)\n",
      "epoch: 250 loss: tensor(5.7689)\n",
      "epoch: 251 loss: tensor(5.7778)\n",
      "epoch: 252 loss: tensor(5.7744)\n",
      "epoch: 253 loss: tensor(5.7719)\n",
      "epoch: 254 loss: tensor(5.7726)\n",
      "epoch: 255 loss: tensor(5.7704)\n",
      "epoch: 256 loss: tensor(5.7669)\n",
      "epoch: 257 loss: tensor(5.7685)\n",
      "epoch: 258 loss: tensor(5.7751)\n",
      "epoch: 259 loss: tensor(5.7624)\n",
      "epoch: 260 loss: tensor(5.7654)\n",
      "epoch: 261 loss: tensor(5.7672)\n",
      "epoch: 262 loss: tensor(5.7697)\n",
      "epoch: 263 loss: tensor(5.7718)\n",
      "epoch: 264 loss: tensor(5.7677)\n",
      "epoch: 265 loss: tensor(5.7718)\n",
      "epoch: 266 loss: tensor(5.7665)\n",
      "epoch: 267 loss: tensor(5.7695)\n",
      "epoch: 268 loss: tensor(5.7620)\n",
      "epoch: 269 loss: tensor(5.7651)\n",
      "epoch: 270 loss: tensor(5.7615)\n",
      "epoch: 271 loss: tensor(5.7662)\n",
      "epoch: 272 loss: tensor(5.7620)\n",
      "epoch: 273 loss: tensor(5.7658)\n",
      "epoch: 274 loss: tensor(5.7697)\n",
      "epoch: 275 loss: tensor(5.7633)\n",
      "epoch: 276 loss: tensor(5.7669)\n",
      "epoch: 277 loss: tensor(5.7670)\n",
      "epoch: 278 loss: tensor(5.7636)\n",
      "epoch: 279 loss: tensor(5.7652)\n",
      "epoch: 280 loss: tensor(5.7640)\n",
      "epoch: 281 loss: tensor(5.7646)\n",
      "epoch: 282 loss: tensor(5.7693)\n",
      "epoch: 283 loss: tensor(5.7672)\n",
      "epoch: 284 loss: tensor(5.7671)\n",
      "epoch: 285 loss: tensor(5.7608)\n",
      "epoch: 286 loss: tensor(5.7587)\n",
      "epoch: 287 loss: tensor(5.7606)\n",
      "epoch: 288 loss: tensor(5.7584)\n",
      "epoch: 289 loss: tensor(5.7591)\n",
      "epoch: 290 loss: tensor(5.7579)\n",
      "epoch: 291 loss: tensor(5.7617)\n",
      "epoch: 292 loss: tensor(5.7637)\n",
      "epoch: 293 loss: tensor(5.7674)\n",
      "epoch: 294 loss: tensor(5.7582)\n",
      "epoch: 295 loss: tensor(5.7533)\n",
      "epoch: 296 loss: tensor(5.7594)\n",
      "epoch: 297 loss: tensor(5.7622)\n",
      "epoch: 298 loss: tensor(5.7623)\n",
      "epoch: 299 loss: tensor(5.7669)\n",
      "epoch: 300 loss: tensor(5.7575)\n",
      "epoch: 301 loss: tensor(5.7639)\n",
      "epoch: 302 loss: tensor(5.7516)\n",
      "epoch: 303 loss: tensor(5.7618)\n",
      "epoch: 304 loss: tensor(5.7643)\n",
      "epoch: 305 loss: tensor(5.7553)\n",
      "epoch: 306 loss: tensor(5.7597)\n",
      "epoch: 307 loss: tensor(5.7646)\n",
      "epoch: 308 loss: tensor(5.7598)\n",
      "epoch: 309 loss: tensor(5.7527)\n",
      "epoch: 310 loss: tensor(5.7552)\n",
      "epoch: 311 loss: tensor(5.7603)\n",
      "epoch: 312 loss: tensor(5.7557)\n",
      "epoch: 313 loss: tensor(5.7573)\n",
      "epoch: 314 loss: tensor(5.7543)\n",
      "epoch: 315 loss: tensor(5.7599)\n",
      "epoch: 316 loss: tensor(5.7518)\n",
      "epoch: 317 loss: tensor(5.7598)\n",
      "epoch: 318 loss: tensor(5.7524)\n",
      "epoch: 319 loss: tensor(5.7539)\n",
      "epoch: 320 loss: tensor(5.7561)\n",
      "epoch: 321 loss: tensor(5.7560)\n",
      "epoch: 322 loss: tensor(5.7515)\n",
      "epoch: 323 loss: tensor(5.7493)\n",
      "epoch: 324 loss: tensor(5.7577)\n",
      "epoch: 325 loss: tensor(5.7456)\n",
      "epoch: 326 loss: tensor(5.7487)\n",
      "epoch: 327 loss: tensor(5.7505)\n",
      "epoch: 328 loss: tensor(5.7483)\n",
      "epoch: 329 loss: tensor(5.7574)\n",
      "epoch: 330 loss: tensor(5.7596)\n",
      "epoch: 331 loss: tensor(5.7502)\n",
      "epoch: 332 loss: tensor(5.7566)\n",
      "epoch: 333 loss: tensor(5.7492)\n",
      "epoch: 334 loss: tensor(5.7499)\n",
      "epoch: 335 loss: tensor(5.7537)\n",
      "epoch: 336 loss: tensor(5.7555)\n",
      "epoch: 337 loss: tensor(5.7571)\n",
      "epoch: 338 loss: tensor(5.7499)\n",
      "epoch: 339 loss: tensor(5.7473)\n",
      "epoch: 341 loss: tensor(5.7579)\n",
      "epoch: 342 loss: tensor(5.7495)\n",
      "epoch: 343 loss: tensor(5.7445)\n",
      "epoch: 344 loss: tensor(5.7485)\n",
      "epoch: 345 loss: tensor(5.7467)\n",
      "epoch: 346 loss: tensor(5.7563)\n",
      "epoch: 347 loss: tensor(5.7489)\n",
      "epoch: 348 loss: tensor(5.7526)\n",
      "epoch: 349 loss: tensor(5.7491)\n",
      "epoch: 350 loss: tensor(5.7568)\n",
      "epoch: 351 loss: tensor(5.7494)\n",
      "epoch: 352 loss: tensor(5.7491)\n",
      "epoch: 353 loss: tensor(5.7461)\n",
      "epoch: 354 loss: tensor(5.7478)\n",
      "epoch: 355 loss: tensor(5.7495)\n",
      "epoch: 356 loss: tensor(5.7405)\n",
      "epoch: 357 loss: tensor(5.7443)\n",
      "epoch: 358 loss: tensor(5.7433)\n",
      "epoch: 359 loss: tensor(5.7445)\n",
      "epoch: 360 loss: tensor(5.7452)\n",
      "epoch: 361 loss: tensor(5.7438)\n",
      "epoch: 362 loss: tensor(5.7510)\n",
      "epoch: 363 loss: tensor(5.7462)\n",
      "epoch: 364 loss: tensor(5.7473)\n",
      "epoch: 365 loss: tensor(5.7503)\n",
      "epoch: 366 loss: tensor(5.7511)\n",
      "epoch: 367 loss: tensor(5.7460)\n",
      "epoch: 368 loss: tensor(5.7428)\n",
      "epoch: 369 loss: tensor(5.7421)\n",
      "epoch: 370 loss: tensor(5.7477)\n",
      "epoch: 371 loss: tensor(5.7443)\n",
      "epoch: 372 loss: tensor(5.7446)\n",
      "epoch: 373 loss: tensor(5.7475)\n",
      "epoch: 374 loss: tensor(5.7468)\n",
      "epoch: 375 loss: tensor(5.7472)\n",
      "epoch: 376 loss: tensor(5.7447)\n",
      "epoch: 377 loss: tensor(5.7474)\n",
      "epoch: 378 loss: tensor(5.7443)\n",
      "epoch: 379 loss: tensor(5.7368)\n",
      "epoch: 380 loss: tensor(5.7446)\n",
      "epoch: 381 loss: tensor(5.7409)\n",
      "epoch: 382 loss: tensor(5.7418)\n",
      "epoch: 383 loss: tensor(5.7463)\n",
      "epoch: 384 loss: tensor(5.7506)\n",
      "epoch: 385 loss: tensor(5.7395)\n",
      "epoch: 386 loss: tensor(5.7388)\n",
      "epoch: 388 loss: tensor(5.7361)\n",
      "epoch: 389 loss: tensor(5.7366)\n",
      "epoch: 390 loss: tensor(5.7442)\n",
      "epoch: 391 loss: tensor(5.7455)\n",
      "epoch: 392 loss: tensor(5.7391)\n",
      "epoch: 393 loss: tensor(5.7426)\n",
      "epoch: 394 loss: tensor(5.7378)\n",
      "epoch: 395 loss: tensor(5.7373)\n",
      "epoch: 396 loss: tensor(5.7458)\n",
      "epoch: 397 loss: tensor(5.7368)\n",
      "epoch: 398 loss: tensor(5.7404)\n",
      "epoch: 399 loss: tensor(5.7371)\n"
     ]
    }
   ],
   "source": [
    "input_length = 16\n",
    "alphabet_size = 4\n",
    "model = AutoEncoder(input_length, alphabet_size, 64)\n",
    "model.cuda()\n",
    "model.zero_grad()\n",
    "\n",
    "loss_fn = torch.nn.MSELoss(reduction='sum')\n",
    "minibatch_size = 100\n",
    "num_epochs = 400\n",
    "num_minibatches = 1000 # len(data_source.train) // minibatch_size\n",
    "\n",
    "learning_rate = 1e-2\n",
    "#optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.1, momentum=0.9)\n",
    "loss_denom = minibatch_size * num_minibatches * input_length * alphabet_size\n",
    "\n",
    "data_source.train.reset()\n",
    "for epoch in range(num_epochs):\n",
    "    epoch_loss = 0\n",
    "    for _ in range(num_minibatches):\n",
    "        batch = data_source.train.next_batch(minibatch_size)\n",
    "        batch = np.reshape(batch, [batch.shape[0], batch.shape[1] * batch.shape[2]])\n",
    "        batch = batch.astype(np.float64)\n",
    "        batch = torch.FloatTensor(batch).cuda()\n",
    "        loss = 0\n",
    "        for seq in batch:\n",
    "            model.zero_grad()\n",
    "            pred = model(seq)\n",
    "            loss += loss_fn(pred, seq)\n",
    "            epoch_loss += loss\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    print(\"epoch:\", epoch, \"loss:\", epoch_loss.cpu().data / loss_denom)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), '/home/dcdanko/Dev/gimmebio/gimmebio/seqtalk/simple_continuous_autoencoder.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Function\n",
    "\n",
    "class Hardsigmoid(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(Hardsigmoid, self).__init__()\n",
    "        self.act = nn.Hardtanh()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return (self.act(x) + 1.0) / 2.0\n",
    "\n",
    "    \n",
    "\n",
    "class BernoulliFunctionST(Function):\n",
    "\n",
    "    @staticmethod\n",
    "    def forward(ctx, input):\n",
    "        return torch.bernoulli(input)\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        return grad_output\n",
    "\n",
    "BernoulliST = BernoulliFunctionST.apply\n",
    "\n",
    "\n",
    "class StochasticBinaryActivation(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(StochasticBinaryActivation, self).__init__()\n",
    "\n",
    "        self.act = Hardsigmoid()\n",
    "        self.slope = 1.0\n",
    "        self.binarizer = BernoulliST\n",
    "\n",
    "    def forward(self, x):\n",
    "        probs = self.act(self.slope * x)\n",
    "        out = self.binarizer(probs)\n",
    "        return out\n",
    "    \n",
    "\n",
    "class BinaryNet(nn.Module):\n",
    "\n",
    "    def __init__(self, input_size, inner_size, outer_size):\n",
    "        super(BinaryNet, self).__init__()\n",
    "\n",
    "        self.fc1 = nn.Linear(input_size, inner_size)\n",
    "        self.fc2 = nn.Linear(inner_size, outer_size)\n",
    "        self.act = StochasticBinaryActivation()\n",
    "\n",
    "    def forward(self, x):\n",
    "        #x = x.view(-1, 784)\n",
    "        x = self.fc1(x).clamp(min=0)\n",
    "        x = x / torch.max(x)\n",
    "        x = self.act(x)\n",
    "        x = self.fc2(x).clamp(min=0)\n",
    "        x = x / torch.max(x)\n",
    "        return x\n",
    "    \n",
    "class SimpleBinaryNet(nn.Module):\n",
    "\n",
    "    def __init__(self, input_size):\n",
    "        super(SimpleBinaryNet, self).__init__()\n",
    "\n",
    "        self.fc1 = nn.Linear(input_size, input_size, bias=False)\n",
    "        self.act = StochasticBinaryActivation()\n",
    "        self.sig = torch.nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        #x = x.view(-1, 784)\n",
    "        x = self.fc1(x)\n",
    "        x = x.clamp(min=0)\n",
    "        x = self.sig(x)\n",
    "        #x = x.clamp(min=0)\n",
    "        #x = x / (torch.max(x) + 0.001)\n",
    "        #x = self.act(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0 loss: tensor(7.3758)\n",
      "epoch: 1 loss: tensor(7.2352)\n"
     ]
    }
   ],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "input_length = 16\n",
    "alphabet_size = 4\n",
    "model = SimpleBinaryNet(64)\n",
    "model.cpu()\n",
    "model.zero_grad()\n",
    "\n",
    "minibatch_size = 100\n",
    "num_epochs = 2\n",
    "num_minibatches = 100 # len(data_source.train) // minibatch_size\n",
    "\n",
    "loss_fn = torch.nn.MSELoss(reduction='sum')\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.5, momentum=0.9)\n",
    "loss_denom = minibatch_size * num_minibatches * input_length * alphabet_size\n",
    "\n",
    "softmax = torch.nn.Softmax(dim=0)\n",
    "data_source.train.reset()\n",
    "for epoch in range(num_epochs):\n",
    "    epoch_loss = 0\n",
    "    for _ in range(num_minibatches):\n",
    "        batch = data_source.train.next_batch(minibatch_size)\n",
    "        batch = np.reshape(batch, [batch.shape[0], batch.shape[1] * batch.shape[2]])\n",
    "        batch = batch.astype(np.float64)\n",
    "        batch = torch.FloatTensor(batch).cpu()\n",
    "        loss = 0\n",
    "        for seq in batch:\n",
    "            model.zero_grad()\n",
    "            pred = model(seq)\n",
    "            rpred, rseq = torch.reshape(pred, [4, 16]), torch.reshape(seq, [4, 16])\n",
    "            rpred = softmax(rpred)\n",
    "            loss += loss_fn(rpred, rseq)\n",
    "            epoch_loss += loss\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    print(\"epoch:\", epoch, \"loss:\", epoch_loss.cpu().data / loss_denom)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1296x216 with 3 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "\n",
    "batch = data_source.test.next_batch(1)\n",
    "batch = np.reshape(batch, [batch.shape[0], batch.shape[1] * batch.shape[2]])\n",
    "batch = torch.FloatTensor(batch).cpu()\n",
    "pred = torch.reshape(model(batch[0]), [4, 16]).detach().numpy()\n",
    "real = torch.reshape(batch[0], [4, 16]).detach().numpy()\n",
    "\n",
    "\n",
    "plt.figure(figsize=(18,3))\n",
    "plt.subplot(3,1,1)\n",
    "plt.matshow(real, vmin=-0.1, vmax=1.1, fignum=False)\n",
    "plt.text(0,6,\"the tensor for the original template\")\n",
    "frame = plt.gca()\n",
    "frame.axes.yaxis.set_ticklabels([])\n",
    "\n",
    "plt.subplot(3,1,2)\n",
    "plt.matshow(pred, vmin=-0.1, vmax=1.1, fignum=False)\n",
    "plt.text(0,6,\"reconstructed tensor from the DCNet\")\n",
    "frame = plt.gca()\n",
    "frame.axes.yaxis.set_ticklabels([])\n",
    "\n",
    "plt.subplot(3,1,3)\n",
    "plt.matshow(real - pred, vmin=-0.1, vmax=1.1, fignum=False)\n",
    "plt.text(0,6,\"differences\")\n",
    "frame = plt.gca()\n",
    "frame.axes.yaxis.set_ticklabels([])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[23.5047,  0.3098, -0.3573,  ...,  0.4429, -0.7860,  0.5565],\n",
       "        [ 0.4131, 22.4170,  0.7066,  ...,  0.3421, -0.1595, -1.4605],\n",
       "        [-0.2162, -0.7536,  1.2968,  ..., -0.7842, -0.3503, -0.8582],\n",
       "        ...,\n",
       "        [-0.4668, -0.3502, -0.3131,  ..., 23.2269,  0.3563,  0.0395],\n",
       "        [-1.1508, -0.2689, -0.3858,  ...,  0.8956, 24.9147,  0.2631],\n",
       "        [ 0.2556, -0.1152,  0.9256,  ..., -0.3547,  0.6694, 20.9087]])"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fc1.weight.detach()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 0., 0., 0., 1., 0., 0., 1., 1., 0., 1., 0., 0., 0., 1., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 1., 0., 1.],\n",
       "        [0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.],\n",
       "        [1., 0., 0., 1., 0., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.]])"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "foo = torch.reshape(batch[0], [4, 16])#.detach().numpy()\n",
    "foo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0407, 0.0407, 0.0407, 0.0407, 0.1105],\n",
       "        [0.0437, 0.0437, 0.0437, 0.0437, 0.0437],\n",
       "        [0.0473, 0.1285, 0.1285, 0.0473, 0.0473],\n",
       "        [0.1188, 0.0437, 0.0437, 0.1188, 0.0437]])"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "softmax = torch.nn.Softmax(dim=1)\n",
    "softmax(foo)[:,0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.5000, 0.5000, 0.5000, 0.5000, 1.0000],\n",
       "        [0.5000, 0.5000, 0.5000, 0.5000, 0.5000],\n",
       "        [0.5000, 1.0000, 1.0000, 0.5000, 0.5000],\n",
       "        [1.0000, 0.5000, 0.5000, 1.0000, 0.5000]], grad_fn=<SliceBackward>)"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bar = torch.reshape(model(batch[0]), [4, 16])\n",
    "bar[:,0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0538, 0.0538, 0.0538, 0.0538, 0.0887],\n",
       "        [0.0538, 0.0538, 0.0538, 0.0538, 0.0538],\n",
       "        [0.0557, 0.0919, 0.0919, 0.0557, 0.0557],\n",
       "        [0.0919, 0.0557, 0.0557, 0.0919, 0.0557]], grad_fn=<SliceBackward>)"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "softmax(bar)[:,0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
